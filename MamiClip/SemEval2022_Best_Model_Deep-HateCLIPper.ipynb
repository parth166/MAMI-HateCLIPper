{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lywjLatsS_k",
        "outputId": "f5bf3258-a970-4675-ed84-44522961ecac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4670928039051208254\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14343274496\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 10710056389999296978\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZXlDffgsWlE",
        "outputId": "1edaee7d-6e72-4c6e-9887-f7cc61230079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.65.0)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.22.4)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2022.10.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.1.1 ujson-5.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from openai-clip) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-clip) (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip) (0.2.6)\n",
            "Building wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368607 sha256=e19180b92fdb14513803b22a57025cf8e0e6af0c59d5c9d8267a6694dd1b2820\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: openai-clip\n",
            "Successfully installed openai-clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install scikit-learn\n",
        "!pip install scipy\n",
        "!pip install ekphrasis\n",
        "!pip install openai-clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frBzayhAsZTL",
        "outputId": "af3c8159-19d8-4257-f0dc-b28567ea603f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_A2WvmeDsbsc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import clip\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn import metrics\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import sys\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Insert the directory\n",
        "sys.path.insert(0,'/content/drive/MyDrive/MamiGit/mami')\n",
        "import random\n",
        "from helper_functions import *\n",
        "from text_normalizer import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kDVY3MqFs4vq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "acb307e1-6c83-4170-ef6a-68c43363acd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pKnT4vpIs5kr"
      },
      "outputs": [],
      "source": [
        "## Dataset Loading\n",
        "tr_df = pd.read_csv('/content/drive/MyDrive/MamiGit/mami/data/train.tsv', sep='\\t') # use '/t' for main training data\n",
        "vl_df = pd.read_csv('/content/drive/MyDrive/MamiGit/mami/data/validation.tsv', sep='\\t') # use '/t' for main training data\n",
        "ts_df = pd.read_csv('/content/drive/MyDrive/MamiGit/mami/data/test.tsv', sep='\\t') # use '/t' for main training data\n",
        "#tr_df = tr_df.head(300)\n",
        "#vl_df = vl_df.head(300)\n",
        "#ts_df = ts_df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GvzEfK_ss_Ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadf471e-d9c4-465e-c259-a7a1c2526891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   file_name  label  shaming  stereotype  objectification  violence  \\\n",
            "0   8716.jpg      1        0           1                1         0   \n",
            "1   3066.jpg      1        1           1                0         0   \n",
            "2   6038.jpg      0        0           0                0         0   \n",
            "3  10861.jpg      1        0           0                1         0   \n",
            "4  11198.jpg      0        0           0                0         0   \n",
            "\n",
            "                                                text  \n",
            "0  GETS MARRIED TO GIRL OF HIS DREAMS SHE DOESN'T...  \n",
            "1  When your mama don't change yo diaper for 19 y...  \n",
            "2  Some people want a big house, a fast car, and ...  \n",
            "3     FAP FAP FAP FAP FAP FAP memecenter Meme Center  \n",
            "4              I RAISE. A I CALL. I FOLD. I'M ALL IN  \n"
          ]
        }
      ],
      "source": [
        "print(tr_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cXmA5DUYtOCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf32659-f1de-42da-c467-1b46c6ccaee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['file_name', 'label', 'shaming', 'stereotype', 'objectification', 'violence', 'text']\n"
          ]
        }
      ],
      "source": [
        "tr_df.rename(columns = {'Text Transcription':'text'}, inplace = True)\n",
        "tr_df.rename(columns = {'misogynous':'label'}, inplace = True)\n",
        "\n",
        "my_list = tr_df.columns.values.tolist()\n",
        "print (my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "454PvoIitPrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac478170-52f4-49af-95f1-bfdc524d0f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  file_name  label  shaming  stereotype  objectification  violence  \\\n",
            "0  1377.jpg      1        0           1                0         0   \n",
            "1  5498.jpg      1        0           1                0         0   \n",
            "2  5697.jpg      1        0           1                0         0   \n",
            "3  5182.jpg      1        0           0                0         0   \n",
            "4  6162.jpg      1        0           1                0         0   \n",
            "\n",
            "                                                text  \n",
            "0  IF WOMEN WANT EQUAL PAY THEY SHOULD DO EQUAL W...  \n",
            "1                  IF A WOMAN WERE TO DRIVE YOUR CAR  \n",
            "2                           When a woman is a driver  \n",
            "3  SEXY BITCHES? Ba GIVE THEM BACON TO MAKE THEM ...  \n",
            "4  When you reach peak feminism FEMINIST Just Bec...  \n"
          ]
        }
      ],
      "source": [
        "print(vl_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jxAs1e7AtVZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a745d313-caa9-4aa2-af53-5d183700c109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of      file_name  label  shaming  stereotype  objectification  violence  \\\n",
            "0    15236.jpg      0        0           0                0         0   \n",
            "1    15805.jpg      1        0           1                1         0   \n",
            "2    16254.jpg      0        0           0                0         0   \n",
            "3    16191.jpg      1        0           1                1         0   \n",
            "4    15952.jpg      0        0           0                0         0   \n",
            "..         ...    ...      ...         ...              ...       ...   \n",
            "995  15591.jpg      1        0           1                1         0   \n",
            "996  15049.jpg      0        0           0                0         0   \n",
            "997  15363.jpg      1        0           1                1         0   \n",
            "998  15199.jpg      0        0           0                0         0   \n",
            "999  15853.jpg      0        0           0                0         0   \n",
            "\n",
            "                                                  text  \n",
            "0    FACEBOOK SINGLES GROUPS BELIKE WHEN A NEW WOMA...  \n",
            "1      SO, IF YOU'RE A FEMINIST HOW CAN YOU EAT DAIRY?  \n",
            "2           WHEN A CUTE GIRL LEFT YOUR MESSAGE ON SEEN  \n",
            "3    Photographing something you want to show every...  \n",
            "4    HEY BABE CAN YOU MAKE ME A SANDWICH? Hey babe ...  \n",
            "..                                                 ...  \n",
            "995  IT'S NOT YOUR FAULT You didn't design the dres...  \n",
            "996  THINK ABOUT HOW MUCH BETTER HER SKIN IS BREATH...  \n",
            "997  THE STEREOTYPES ARE TRUE F SHE DOES HAVE A TIG...  \n",
            "998  DRAWS NAKED PICTURES OF BLACK WOMEN 00 0000 GE...  \n",
            "999  \"You work too much.\" Me: OOL I want to be rich...  \n",
            "\n",
            "[1000 rows x 7 columns]>\n"
          ]
        }
      ],
      "source": [
        "print(ts_df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MJGzfNnZtV9W"
      },
      "outputs": [],
      "source": [
        "#seed = 42\n",
        "#random.seed(seed)\n",
        "#np.random.seed(seed)\n",
        "#torch.manual_seed(seed)\n",
        "#torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xCi2NSbPtY9M"
      },
      "outputs": [],
      "source": [
        "# Arguments\n",
        "batch_size=64\n",
        "init_lr=1e-4\n",
        "epochs=10\n",
        "max_length=77\n",
        "lr=1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LrbRHuH3taB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ac4055-b98e-420f-ec80-9da275180bb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 890M/890M [00:03<00:00, 258MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import clip\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Pre-trained Stream Models\n",
        "clip_nms = {'vit32': 'ViT-B/32', 'vit16': 'ViT-B/16', 'rn50': 'RN50', 'rn504': 'RN50x4', 'rn101': 'RN101', 'rn5016': 'RN50x16', 'rn5064': 'RN50x64', 'vit14': 'ViT-L/14'}\n",
        "clip_dim = {'vit32': 512, 'vit16': 512, 'vit14': 768, 'rn50': 1024, 'rn504': 640, 'rn101': 512, 'rn5016': 768, 'rn5064': 1024}\n",
        "\n",
        "vmodel='vit14' # Choose the desired model\n",
        "\n",
        "# Load the pre-trained CLIP model\n",
        "clip_model, _ = clip.load(clip_nms[vmodel], jit=False)\n",
        "input_resolution = clip_model.visual.input_resolution\n",
        "\n",
        "# Set the model to float and evaluation mode\n",
        "clip_model = clip_model.float().eval()\n",
        "\n",
        "# Wrap the model with DataParallel for multi-GPU training if available\n",
        "clip_model = nn.DataParallel(clip_model)\n",
        "\n",
        "# Get the dimension for the chosen model\n",
        "dim = clip_dim[vmodel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2pEC-evfuY9U"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "input_resolution = 224  # Specify the desired input resolution\n",
        "\n",
        "transform_config = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_resolution, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "            std=[0.26862954, 0.26130258, 0.27577711]\n",
        "        )\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((input_resolution, input_resolution), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "            std=[0.26862954, 0.26130258, 0.27577711]\n",
        "        )\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fdRTIPWquaI-"
      },
      "outputs": [],
      "source": [
        "# !pip install simpletokenizer\n",
        "from clip.simple_tokenizer import SimpleTokenizer\n",
        "import torch\n",
        "\n",
        "_tokenizer = SimpleTokenizer()\n",
        "\n",
        "def tokenize(text, context_length: int = 77):\n",
        "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
        "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
        "\n",
        "    tokens = [sot_token] + _tokenizer.encode(text)[:context_length-2] + [eot_token]\n",
        "    result = torch.zeros(context_length, dtype=torch.long)\n",
        "    mask = torch.zeros(context_length, dtype=torch.long)\n",
        "    result[:len(tokens)] = torch.tensor(tokens)\n",
        "    mask[:len(tokens)] = 1\n",
        "\n",
        "    return result, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5wSeE0F5vBHt"
      },
      "outputs": [],
      "source": [
        "train_data = CustomDatasetFixed(tr_df, 'training', transform_config['test'], preprocess, tokenize, max_length)\n",
        "valid_data = CustomDatasetFixed(vl_df, 'training', transform_config['test'], preprocess, tokenize, max_length)\n",
        "test_data = CustomDatasetFixed(ts_df, 'test', transform_config['test'], preprocess, tokenize, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nNzV6riovSAl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MMNetwork(nn.Module):\n",
        "    def __init__(self, vdim, tdim, n_cls):\n",
        "        super(MMNetwork, self).__init__()\n",
        "\n",
        "        self.vfc = nn.Linear(vdim, 512)\n",
        "        self.tfc = nn.Linear(vdim, 512)\n",
        "\n",
        "        self.pre_output_layer1 = nn.Linear(512, 1024)\n",
        "        self.pre_output_layer2 = nn.Linear(1024, 1024)\n",
        "        self.pre_output_layer3 = nn.Linear(1024, 1024)\n",
        "        self.pre_output_layer4 = nn.Linear(1024, 64)\n",
        "\n",
        "\n",
        "        self.cf1 = nn.Linear(64, 1)\n",
        "        self.cf2 = nn.Linear(64, 1)\n",
        "        self.cf3 = nn.Linear(64, 1)\n",
        "        self.cf4 = nn.Linear(64, 1)\n",
        "        self.cf5 = nn.Linear(64, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, vx, tx, masks=None):\n",
        "        fi = vx\n",
        "        ft = tx\n",
        "        \n",
        "        pi = self.dropout(self.relu(self.vfc(fi)))\n",
        "        pt = self.dropout(self.relu(self.tfc(ft)))\n",
        "\n",
        "        fim = pi * pt\n",
        "\n",
        "        output = self.dropout(self.relu(self.pre_output_layer1(fim)))\n",
        "        output = self.dropout(self.relu(self.pre_output_layer2(output)))\n",
        "        output = self.dropout(self.relu(self.pre_output_layer3(output)))\n",
        "        output = self.dropout(self.relu(self.pre_output_layer4(output)))\n",
        "\n",
        "        output1 = torch.sigmoid(self.dropout(self.relu(self.cf1(output))))\n",
        "        output2 = torch.sigmoid(self.dropout(self.relu(self.cf2(output))))\n",
        "        output3 = torch.sigmoid(self.dropout(self.relu(self.cf3(output))))\n",
        "        output4 = torch.sigmoid(self.dropout(self.relu(self.cf4(output))))\n",
        "        output5 = torch.sigmoid(self.dropout(self.relu(self.cf5(output))))\n",
        "\n",
        "        return output1, output2, output3, output4, output5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZvzwlSErIJVs"
      },
      "outputs": [],
      "source": [
        "def performErrorHandling(all_preds1, all_labels1):\n",
        "\n",
        "  with open('/content/drive/MyDrive/non_matching_indexes_64_10_1e4-final-train.txt', 'w') as f:\n",
        "            # Loop through the indexes of the labels in the first list\n",
        "            for i in range(len(all_labels1)): \n",
        "              # Check if the label in the first list does not match the label in the second list\n",
        "              if all_labels1[i] != all_preds1[i]:\n",
        "                # Write the index to the file\n",
        "                f.write(str(i) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qhKgnhpkvSI2"
      },
      "outputs": [],
      "source": [
        "all_preds1 = []\n",
        "all_labels1 = []\n",
        "all_preds2 = []\n",
        "all_labels2 = []\n",
        "all_preds3 = []\n",
        "all_labels3 = []\n",
        "all_preds4 = []\n",
        "all_labels4 = []\n",
        "all_preds5 = []\n",
        "all_labels5 = []\n",
        "\n",
        "def evaluate(model, loader, isTest):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 in loader:\n",
        "\n",
        "            img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 = img_inps.to(device), \\\n",
        "                txt_tokens.to(device), masks.to(device), labels1.to(device), labels2.to(device), labels3.to(device), \\\n",
        "                labels4.to(device), labels5.to(device)\n",
        "\n",
        "            img_feats = clip_model.module.encode_image(img_inps)\n",
        "            txt_feats = clip_model.module.encode_text(txt_tokens)\n",
        "\n",
        "            outputs1, outputs2, outputs3, outputs4, outputs5 = model(img_feats, txt_feats, masks)\n",
        "\n",
        "            preds1 = (outputs1>0.5).int()\n",
        "            preds2 = (outputs2>0.5).int()\n",
        "            preds3 = (outputs3>0.5).int()\n",
        "            preds4 = (outputs4>0.5).int()\n",
        "            preds5 = (outputs5>0.5).int()\n",
        "\n",
        "            test_loss += (criterion(outputs1, labels1.unsqueeze(1).float()).item() + criterion(outputs2, labels2.unsqueeze(1).float()).item() + \\\n",
        "                            criterion(outputs3, labels3.unsqueeze(1).float()).item() + criterion(outputs4, labels4.unsqueeze(1).float()).item() + \\\n",
        "                            criterion(outputs5, labels5.unsqueeze(1).float()).item())\n",
        "\n",
        "            all_preds1.extend(preds1.cpu().numpy().flatten())\n",
        "            all_labels1.extend(labels1.cpu().numpy().flatten())\n",
        "            all_preds2.extend(preds2.cpu().numpy().flatten())\n",
        "            all_labels2.extend(labels2.cpu().numpy().flatten())\n",
        "            all_preds3.extend(preds3.cpu().numpy().flatten())\n",
        "            all_labels3.extend(labels3.cpu().numpy().flatten())\n",
        "            all_preds4.extend(preds4.cpu().numpy().flatten())\n",
        "            all_labels4.extend(labels4.cpu().numpy().flatten())\n",
        "            all_preds5.extend(preds5.cpu().numpy().flatten())\n",
        "            all_labels5.extend(labels5.cpu().numpy().flatten())\n",
        "\n",
        "        acc = metrics.accuracy_score(all_labels1, all_preds1)\n",
        "        f1_1 = metrics.f1_score(all_labels1, all_preds1, average='macro')\n",
        "        f1_2 = metrics.f1_score(all_labels2, all_preds2, average='micro')\n",
        "        f1_3 = metrics.f1_score(all_labels3, all_preds3, average='micro')\n",
        "        f1_4 = metrics.f1_score(all_labels4, all_preds4, average='micro')\n",
        "        f1_5 = metrics.f1_score(all_labels5, all_preds5, average='micro')\n",
        "\n",
        "        if(isTest):\n",
        "          print(\"Performing Error Handling Next - \")\n",
        "          performErrorHandling(all_preds1, all_labels1)\n",
        "\n",
        "    return test_loss/len(loader), acc, f1_1, f1_2, f1_3, f1_4, f1_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4xL0i5Ug9n8Z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset, sampler\n",
        "train_loader = DataLoader(train_data, shuffle=True, num_workers=2, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, num_workers=2, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, num_workers=2, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "x3aYbBYbv5YT"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, lr_scheduler, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model = model\n",
        "    best_acc = 0.0\n",
        "    best_val_loss = 100\n",
        "    best_epoch = 0\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        since2 = time.time()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_samples = 0.0\n",
        "        cnt = 0\n",
        "\n",
        "        for img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 in tr_loader:\n",
        "            img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 = (\n",
        "                img_inps.to(device),\n",
        "                txt_tokens.to(device),\n",
        "                masks.to(device),\n",
        "                labels1.to(device),\n",
        "                labels2.to(device),\n",
        "                labels3.to(device),\n",
        "                labels4.to(device),\n",
        "                labels5.to(device),\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                img_feats = clip_model.module.encode_image(img_inps)\n",
        "                txt_feats = clip_model.module.encode_text(txt_tokens)\n",
        "\n",
        "            outputs1, outputs2, outputs3, outputs4, outputs5 = model(\n",
        "                img_feats, txt_feats, masks\n",
        "            )\n",
        "            preds1 = (outputs1 > 0.5).int()\n",
        "\n",
        "            loss = criterion(outputs1, labels1.unsqueeze(1).float()) + criterion(\n",
        "                outputs2, labels2.unsqueeze(1).float()\n",
        "            ) + criterion(outputs3, labels3.unsqueeze(1).float()) + criterion(\n",
        "                outputs4, labels4.unsqueeze(1).float()\n",
        "            ) + criterion(outputs5, labels5.unsqueeze(1).float())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(preds1 == labels1.data.view_as(preds1)).item()\n",
        "            total_samples += len(labels1)\n",
        "\n",
        "            if cnt % 40 == 0:\n",
        "                print(\n",
        "                    f\"[{epoch}, {cnt+1}] loss: {loss.item():.4f}, Acc: {(100.0 * running_corrects) / total_samples:.2f}\"\n",
        "                )\n",
        "\n",
        "            cnt += 1\n",
        "\n",
        "        if scheduler:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        train_loss = running_loss / len(tr_loader)\n",
        "        train_acc = running_corrects / len(tr_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Training Loss: {train_loss:.6f} Acc: {100.0 * train_acc:.2f}\"\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc, test_f1_1, test_f1_2, test_f1_3, test_f1_4, test_f1_5 = evaluate(\n",
        "            model, vl_loader, False\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, Val Loss: {test_loss:.4f}, Acc: {test_acc*100:.2f}, F1_1: {test_f1_1*100:.2f}, F1_2: {test_f1_2*100:.2f}, F1_3: {test_f1_3*100:.2f}, F1_4: {test_f1_4*100:.2f}, F1_5: {test_f1_5*100:.2f}\"\n",
        "        )\n",
        "\n",
        "        # deep copy the model\n",
        "        if test_f1_1 >= best_f1:\n",
        "            best_acc = test_acc\n",
        "            best_val_loss = test_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "            best_f1 = test_f1_1\n",
        "\n",
        "    time_elapsed2 = time.time() - since2\n",
        "    print('Epoch complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed2 // 60, time_elapsed2 % 60))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    return best_model, best_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1JXvYl9yAEYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079b65f5-0a10-42de-d04e-3d106c55f708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMNetwork(\n",
            "  (vfc): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (tfc): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (pre_output_layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "  (pre_output_layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (pre_output_layer3): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (pre_output_layer4): Linear(in_features=1024, out_features=64, bias=True)\n",
            "  (cf1): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf3): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf5): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "1400 140\n",
            "Epoch 1/10\n",
            "----------\n",
            "[1, 1] loss: 3.4838, Acc: 46.88\n",
            "[1, 41] loss: 3.4638, Acc: 50.99\n",
            "[1, 81] loss: 3.4251, Acc: 51.70\n",
            "[1, 121] loss: 3.4040, Acc: 56.66\n",
            "Training Loss: 3.429751 Acc: 58.92\n",
            "Epoch: 1, Val Loss: 3.3176, Acc: 83.80, F1_1: 83.80, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 2/10\n",
            "----------\n",
            "[2, 1] loss: 3.3787, Acc: 79.69\n",
            "[2, 41] loss: 3.2551, Acc: 76.64\n",
            "[2, 81] loss: 3.3414, Acc: 76.12\n",
            "[2, 121] loss: 3.3997, Acc: 76.77\n",
            "Training Loss: 3.343730 Acc: 76.67\n",
            "Epoch: 2, Val Loss: 3.2790, Acc: 85.15, F1_1: 85.15, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 3/10\n",
            "----------\n",
            "[3, 1] loss: 3.3491, Acc: 70.31\n",
            "[3, 41] loss: 3.3149, Acc: 77.67\n",
            "[3, 81] loss: 3.4388, Acc: 77.97\n",
            "[3, 121] loss: 3.3398, Acc: 78.40\n",
            "Training Loss: 3.318282 Acc: 78.61\n",
            "Epoch: 3, Val Loss: 3.2752, Acc: 85.53, F1_1: 85.53, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 4/10\n",
            "----------\n",
            "[4, 1] loss: 3.2337, Acc: 82.81\n",
            "[4, 41] loss: 3.2640, Acc: 80.60\n",
            "[4, 81] loss: 3.2671, Acc: 80.27\n",
            "[4, 121] loss: 3.2526, Acc: 80.44\n",
            "Training Loss: 3.301685 Acc: 80.68\n",
            "Epoch: 4, Val Loss: 3.2630, Acc: 85.88, F1_1: 85.87, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 5/10\n",
            "----------\n",
            "[5, 1] loss: 3.2127, Acc: 87.50\n",
            "[5, 41] loss: 3.2625, Acc: 79.73\n",
            "[5, 81] loss: 3.2873, Acc: 80.44\n",
            "[5, 121] loss: 3.2961, Acc: 81.07\n",
            "Training Loss: 3.292670 Acc: 81.17\n",
            "Epoch: 5, Val Loss: 3.2581, Acc: 86.12, F1_1: 86.12, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 6/10\n",
            "----------\n",
            "[6, 1] loss: 3.3096, Acc: 82.81\n",
            "[6, 41] loss: 3.2591, Acc: 81.71\n",
            "[6, 81] loss: 3.3775, Acc: 82.14\n",
            "[6, 121] loss: 3.3066, Acc: 82.35\n",
            "Training Loss: 3.275169 Acc: 82.40\n",
            "Epoch: 6, Val Loss: 3.2552, Acc: 86.33, F1_1: 86.33, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 7/10\n",
            "----------\n",
            "[7, 1] loss: 3.1955, Acc: 84.38\n",
            "[7, 41] loss: 3.2620, Acc: 81.55\n",
            "[7, 81] loss: 3.1969, Acc: 81.58\n",
            "[7, 121] loss: 3.2670, Acc: 82.15\n",
            "Training Loss: 3.272896 Acc: 82.17\n",
            "Epoch: 7, Val Loss: 3.2537, Acc: 86.46, F1_1: 86.46, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 8/10\n",
            "----------\n",
            "[8, 1] loss: 3.1865, Acc: 90.62\n",
            "[8, 41] loss: 3.2436, Acc: 81.36\n",
            "[8, 81] loss: 3.3130, Acc: 82.48\n",
            "[8, 121] loss: 3.2512, Acc: 82.44\n",
            "Training Loss: 3.262999 Acc: 82.54\n",
            "Epoch: 8, Val Loss: 3.2542, Acc: 86.54, F1_1: 86.54, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 9/10\n",
            "----------\n",
            "[9, 1] loss: 3.2264, Acc: 92.19\n",
            "[9, 41] loss: 3.2129, Acc: 84.30\n",
            "[9, 81] loss: 3.2216, Acc: 84.09\n",
            "[9, 121] loss: 3.1948, Acc: 83.82\n",
            "Training Loss: 3.254134 Acc: 83.83\n",
            "Epoch: 9, Val Loss: 3.2596, Acc: 86.64, F1_1: 86.64, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 10/10\n",
            "----------\n",
            "[10, 1] loss: 3.2139, Acc: 85.94\n",
            "[10, 41] loss: 3.1894, Acc: 84.18\n",
            "[10, 81] loss: 3.2428, Acc: 83.72\n",
            "[10, 121] loss: 3.1842, Acc: 84.09\n",
            "Training Loss: 3.248746 Acc: 84.06\n",
            "Epoch: 10, Val Loss: 3.2749, Acc: 86.74, F1_1: 86.74, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch complete in 8m 34s\n",
            "Training complete in 95m 6s\n",
            "--- 5706.343035936356 seconds ---\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "dim = clip_dim[vmodel]\n",
        "# Data Loaders\n",
        "tr_loader = DataLoader(train_data, shuffle=True, num_workers=4, batch_size=batch_size)\n",
        "vl_loader = DataLoader(valid_data, num_workers=2, batch_size=batch_size)\n",
        "ts_loader = DataLoader(test_data, num_workers=2, batch_size=batch_size)\n",
        "\n",
        "# Model\n",
        "model = MMNetwork(dim, dim, 1).to(device)\n",
        "print(model)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), init_lr, betas=(0.99, 0.98), weight_decay=1e-4)\n",
        "criterion = nn.BCELoss()\n",
        "num_train_steps = len(train_data) // batch_size * epochs\n",
        "num_warmup_steps = int(0.1 * num_train_steps)\n",
        "print(num_train_steps, num_warmup_steps)  # Print Number of total and warmup steps\n",
        "\n",
        "scheduler = MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.5)\n",
        "import time\n",
        "start_time = time.time()\n",
        "model_ft, best_epoch = train(model, optimizer, scheduler, num_epochs=epochs)\n",
        "#torch.save(model_ft.state_dict(), f\"saved_models/trained_model_epoch_{epoch}.pt\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# torch.save(model_ft.state_dict(), 'saved_models/trained_model_%s.pt'%(args.net, vmodel))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds1 = []\n",
        "all_labels1 = []\n",
        "all_preds2 = []\n",
        "all_labels2 = []\n",
        "all_preds3 = []\n",
        "all_labels3 = []\n",
        "all_preds4 = []\n",
        "all_labels4 = []\n",
        "all_preds5 = []\n",
        "all_labels5 = []"
      ],
      "metadata": {
        "id": "Hp98mqql1fkv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vl_loss, vl_acc, vl_f1, _, _, _, _ = evaluate(model_ft, vl_loader, False)\n",
        "print('Validation best epoch: %d, Val Loss: %.4f, ACC: %.2f, F1: %.2f' % (best_epoch, np.round(vl_loss, 4), vl_acc*100, vl_f1*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwfTfj37jDL2",
        "outputId": "08add168-4d3b-46f2-a0e7-37c615e546a8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation best epoch: 10, Val Loss: 3.2749, ACC: 83.03, F1: 82.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PKViEIrXwUYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d281c28a-90dd-4e6a-b72d-b21f2a46e6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Error Handling Next - \n",
            "0.8053746424390955\n",
            "0.8655\n",
            "0.687\n",
            "0.712\n",
            "0.8735\n"
          ]
        }
      ],
      "source": [
        "ts_loss, ts_acc, ts_f1, ts_f2, ts_f3, ts_f4, ts_f5 = evaluate(model_ft, test_loader, True)\n",
        "\n",
        "print(ts_f1)\n",
        "print(ts_f2)\n",
        "print(ts_f3)\n",
        "print(ts_f4)\n",
        "print(ts_f5)\n",
        "\n",
        "weighted_f1_scores = [ts_f2, ts_f3, ts_f4, ts_f5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_ones2 = all_labels2.count(1)\n",
        "count_ones3 = all_labels3.count(1)\n",
        "count_ones4 = all_labels4.count(1)\n",
        "count_ones5 = all_labels5.count(1)\n",
        "\n",
        "class_frequencies = [count_ones2, count_ones3 , count_ones4, count_ones5]\n",
        "weights = [0, 0, 0, 0]\n",
        "total = 0\n",
        "\n",
        "for freq in class_frequencies:\n",
        "  total += freq\n",
        "\n",
        "for i in range(len(class_frequencies)):\n",
        "  weights[i] = class_frequencies[i]/total\n",
        "  print(\"Weights for class \" + str(i+1) + \" are \" + str(weights[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQcHfNGRM6c0",
        "outputId": "2d0f0bc9-6ab5-494c-aa46-9888ed5e3940"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for class 1 are 0.15603248259860789\n",
            "Weights for class 2 are 0.36310904872389793\n",
            "Weights for class 3 are 0.33410672853828305\n",
            "Weights for class 4 are 0.14675174013921113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_f1_score_task_B = 0\n",
        "\n",
        "for i in range(len(weights)):\n",
        "  weighted_f1_score_task_B += weights[i] * weighted_f1_scores[i]\n",
        "\n",
        "print(weighted_f1_score_task_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5fYPdQ_L7_Z",
        "outputId": "6912c801-a24c-4f37-cd6b-cd7fe8de7d49"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7505736658932715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test results:, Test Loss: %.4f, ACC: %.2f, F1- Task A: %.2f, Weighted F1 - Task B: %.2f'%(np.round(ts_loss,4), ts_acc*100, ts_f1*100, weighted_f1_score_task_B*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QLD5tIJKg0X",
        "outputId": "b3cfa65e-013a-48d7-f452-1730942c4404"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test results:, Test Loss: 3.7632, ACC: 80.75, F1- Task A: 80.54, Weighted F1 - Task B: 75.06\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}