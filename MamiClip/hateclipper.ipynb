{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lywjLatsS_k",
        "outputId": "1cf20234-3661-4f98-8adf-7fe6bd569ee4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4273708771977138024\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14343274496\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 13145887071927280808\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZXlDffgsWlE",
        "outputId": "2a20be76-38b8-46b2-ce09-1dc0755280dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.65.0)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.22.4)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2022.10.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.1.1 ujson-5.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai-clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from openai-clip) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-clip) (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip) (0.2.6)\n",
            "Building wheels for collected packages: openai-clip\n",
            "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368607 sha256=2f575cff73e2dafdf597260e64880eb07ffdc6502963bd220c0ce18665513900\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\n",
            "Successfully built openai-clip\n",
            "Installing collected packages: openai-clip\n",
            "Successfully installed openai-clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install scikit-learn\n",
        "!pip install scipy\n",
        "!pip install ekphrasis\n",
        "!pip install openai-clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frBzayhAsZTL",
        "outputId": "c0e493bb-7d6c-4506-aaf6-add8f3240d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_A2WvmeDsbsc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import clip\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn import metrics\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import sys\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Insert the directory\n",
        "sys.path.insert(0,'/content/drive/MyDrive/MamiGit/mami')\n",
        "import random\n",
        "from helper_functions import *\n",
        "from text_normalizer import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kDVY3MqFs4vq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2f228833-67e4-4297-ffc7-d4cb7830dadc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pKnT4vpIs5kr"
      },
      "outputs": [],
      "source": [
        "## Dataset Loading\n",
        "tr_df = pd.read_csv('/content/drive/MyDrive/MamiGit/mami/data/train.tsv', sep='\\t') # use '/t' for main training data\n",
        "vl_df = pd.read_csv('/content/drive/MyDrive/MamiGit/mami/data/validation.tsv', sep='\\t') # use '/t' for main training data\n",
        "ts_df = pd.read_csv('/content/drive/MyDrive/MamiGit/mami/data/test.tsv', sep='\\t') # use '/t' for main training data\n",
        "#tr_df = tr_df.head(300)\n",
        "#vl_df = vl_df.head(300)\n",
        "#ts_df = ts_df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GvzEfK_ss_Ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4116fea9-35f2-4ac3-de51-86154d33ba8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   file_name  label  shaming  stereotype  objectification  violence  \\\n",
            "0   8716.jpg      1        0           1                1         0   \n",
            "1   3066.jpg      1        1           1                0         0   \n",
            "2   6038.jpg      0        0           0                0         0   \n",
            "3  10861.jpg      1        0           0                1         0   \n",
            "4  11198.jpg      0        0           0                0         0   \n",
            "\n",
            "                                                text  \n",
            "0  GETS MARRIED TO GIRL OF HIS DREAMS SHE DOESN'T...  \n",
            "1  When your mama don't change yo diaper for 19 y...  \n",
            "2  Some people want a big house, a fast car, and ...  \n",
            "3     FAP FAP FAP FAP FAP FAP memecenter Meme Center  \n",
            "4              I RAISE. A I CALL. I FOLD. I'M ALL IN  \n"
          ]
        }
      ],
      "source": [
        "print(tr_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cXmA5DUYtOCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ddebbd-8507-43e3-c940-0aa901af1c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['file_name', 'label', 'shaming', 'stereotype', 'objectification', 'violence', 'text']\n"
          ]
        }
      ],
      "source": [
        "tr_df.rename(columns = {'Text Transcription':'text'}, inplace = True)\n",
        "tr_df.rename(columns = {'misogynous':'label'}, inplace = True)\n",
        "\n",
        "my_list = tr_df.columns.values.tolist()\n",
        "print (my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "454PvoIitPrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a19be34-e6ca-4f86-d699-3341bb8f6c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  file_name  label  shaming  stereotype  objectification  violence  \\\n",
            "0  1377.jpg      1        0           1                0         0   \n",
            "1  5498.jpg      1        0           1                0         0   \n",
            "2  5697.jpg      1        0           1                0         0   \n",
            "3  5182.jpg      1        0           0                0         0   \n",
            "4  6162.jpg      1        0           1                0         0   \n",
            "\n",
            "                                                text  \n",
            "0  IF WOMEN WANT EQUAL PAY THEY SHOULD DO EQUAL W...  \n",
            "1                  IF A WOMAN WERE TO DRIVE YOUR CAR  \n",
            "2                           When a woman is a driver  \n",
            "3  SEXY BITCHES? Ba GIVE THEM BACON TO MAKE THEM ...  \n",
            "4  When you reach peak feminism FEMINIST Just Bec...  \n"
          ]
        }
      ],
      "source": [
        "print(vl_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jxAs1e7AtVZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bf449d-a9bc-4bbb-baae-3c0342818116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of      file_name  label  shaming  stereotype  objectification  violence  \\\n",
            "0    15236.jpg      0        0           0                0         0   \n",
            "1    15805.jpg      1        0           1                1         0   \n",
            "2    16254.jpg      0        0           0                0         0   \n",
            "3    16191.jpg      1        0           1                1         0   \n",
            "4    15952.jpg      0        0           0                0         0   \n",
            "..         ...    ...      ...         ...              ...       ...   \n",
            "995  15591.jpg      1        0           1                1         0   \n",
            "996  15049.jpg      0        0           0                0         0   \n",
            "997  15363.jpg      1        0           1                1         0   \n",
            "998  15199.jpg      0        0           0                0         0   \n",
            "999  15853.jpg      0        0           0                0         0   \n",
            "\n",
            "                                                  text  \n",
            "0    FACEBOOK SINGLES GROUPS BELIKE WHEN A NEW WOMA...  \n",
            "1      SO, IF YOU'RE A FEMINIST HOW CAN YOU EAT DAIRY?  \n",
            "2           WHEN A CUTE GIRL LEFT YOUR MESSAGE ON SEEN  \n",
            "3    Photographing something you want to show every...  \n",
            "4    HEY BABE CAN YOU MAKE ME A SANDWICH? Hey babe ...  \n",
            "..                                                 ...  \n",
            "995  IT'S NOT YOUR FAULT You didn't design the dres...  \n",
            "996  THINK ABOUT HOW MUCH BETTER HER SKIN IS BREATH...  \n",
            "997  THE STEREOTYPES ARE TRUE F SHE DOES HAVE A TIG...  \n",
            "998  DRAWS NAKED PICTURES OF BLACK WOMEN 00 0000 GE...  \n",
            "999  \"You work too much.\" Me: OOL I want to be rich...  \n",
            "\n",
            "[1000 rows x 7 columns]>\n"
          ]
        }
      ],
      "source": [
        "print(ts_df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MJGzfNnZtV9W"
      },
      "outputs": [],
      "source": [
        "#seed = 42\n",
        "#random.seed(seed)\n",
        "#np.random.seed(seed)\n",
        "#torch.manual_seed(seed)\n",
        "#torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xCi2NSbPtY9M"
      },
      "outputs": [],
      "source": [
        "# Arguments\n",
        "batch_size=64\n",
        "init_lr=1e-4\n",
        "epochs=20\n",
        "max_length=77\n",
        "lr=1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LrbRHuH3taB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45dd35d8-6014-4caa-a1d9-c62efb06e0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 890M/890M [00:08<00:00, 104MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import clip\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Pre-trained Stream Models\n",
        "clip_nms = {'vit32': 'ViT-B/32', 'vit16': 'ViT-B/16', 'rn50': 'RN50', 'rn504': 'RN50x4', 'rn101': 'RN101', 'rn5016': 'RN50x16', 'rn5064': 'RN50x64', 'vit14': 'ViT-L/14'}\n",
        "clip_dim = {'vit32': 512, 'vit16': 512, 'vit14': 768, 'rn50': 1024, 'rn504': 640, 'rn101': 512, 'rn5016': 768, 'rn5064': 1024}\n",
        "\n",
        "vmodel='vit14' # Choose the desired model\n",
        "\n",
        "# Load the pre-trained CLIP model\n",
        "clip_model, _ = clip.load(clip_nms[vmodel], jit=False)\n",
        "input_resolution = clip_model.visual.input_resolution\n",
        "\n",
        "# Set the model to float and evaluation mode\n",
        "clip_model = clip_model.float().eval()\n",
        "\n",
        "# Wrap the model with DataParallel for multi-GPU training if available\n",
        "clip_model = nn.DataParallel(clip_model)\n",
        "\n",
        "# Get the dimension for the chosen model\n",
        "dim = clip_dim[vmodel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2pEC-evfuY9U"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "input_resolution = 224  # Specify the desired input resolution\n",
        "\n",
        "transform_config = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_resolution, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "            std=[0.26862954, 0.26130258, 0.27577711]\n",
        "        )\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((input_resolution, input_resolution), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "            std=[0.26862954, 0.26130258, 0.27577711]\n",
        "        )\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fdRTIPWquaI-"
      },
      "outputs": [],
      "source": [
        "# !pip install simpletokenizer\n",
        "from clip.simple_tokenizer import SimpleTokenizer\n",
        "import torch\n",
        "\n",
        "_tokenizer = SimpleTokenizer()\n",
        "\n",
        "def tokenize(text, context_length: int = 77):\n",
        "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
        "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
        "\n",
        "    tokens = [sot_token] + _tokenizer.encode(text)[:context_length-2] + [eot_token]\n",
        "    result = torch.zeros(context_length, dtype=torch.long)\n",
        "    mask = torch.zeros(context_length, dtype=torch.long)\n",
        "    result[:len(tokens)] = torch.tensor(tokens)\n",
        "    mask[:len(tokens)] = 1\n",
        "\n",
        "    return result, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5wSeE0F5vBHt"
      },
      "outputs": [],
      "source": [
        "train_data = CustomDatasetFixed(tr_df, 'training', transform_config['test'], preprocess, tokenize, max_length)\n",
        "valid_data = CustomDatasetFixed(vl_df, 'training', transform_config['test'], preprocess, tokenize, max_length)\n",
        "test_data = CustomDatasetFixed(ts_df, 'test', transform_config['test'], preprocess, tokenize, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nNzV6riovSAl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MMNetwork(nn.Module):\n",
        "    def __init__(self, vdim, tdim, n_cls):\n",
        "        super(MMNetwork, self).__init__()\n",
        "\n",
        "        self.vfc = nn.Linear(vdim, 512)\n",
        "        self.tfc = nn.Linear(vdim, 512)\n",
        "\n",
        "        self.pre_output_layer1 = nn.Linear(512, 1024)\n",
        "        self.pre_output_layer2 = nn.Linear(1024, 64)\n",
        "\n",
        "        self.cf1 = nn.Linear(64, 1)\n",
        "        self.cf2 = nn.Linear(64, 1)\n",
        "        self.cf3 = nn.Linear(64, 1)\n",
        "        self.cf4 = nn.Linear(64, 1)\n",
        "        self.cf5 = nn.Linear(64, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, vx, tx, masks=None):\n",
        "        fi = vx\n",
        "        ft = tx\n",
        "        \n",
        "        pi = self.dropout(self.relu(self.vfc(fi)))\n",
        "        pt = self.dropout(self.relu(self.tfc(ft)))\n",
        "        fim = pi * pt\n",
        "\n",
        "        output = self.dropout(self.relu(self.pre_output_layer1(fim)))\n",
        "        output = self.dropout(self.relu(self.pre_output_layer2(output)))\n",
        "\n",
        "        output1 = torch.sigmoid(self.dropout(self.relu(self.cf1(output))))\n",
        "        output2 = torch.sigmoid(self.dropout(self.relu(self.cf2(output))))\n",
        "        output3 = torch.sigmoid(self.dropout(self.relu(self.cf3(output))))\n",
        "        output4 = torch.sigmoid(self.dropout(self.relu(self.cf4(output))))\n",
        "        output5 = torch.sigmoid(self.dropout(self.relu(self.cf5(output))))\n",
        "\n",
        "        return output1, output2, output3, output4, output5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZvzwlSErIJVs"
      },
      "outputs": [],
      "source": [
        "def performErrorHandling(all_preds1, all_labels1):\n",
        "\n",
        "  with open('/content/drive/MyDrive/non_matching_indexes_20_1e4.txt', 'w') as f:\n",
        "            # Loop through the indexes of the labels in the first list\n",
        "            for i in range(len(all_labels1)): \n",
        "              # Check if the label in the first list does not match the label in the second list\n",
        "              if all_labels1[i] != all_preds1[i]:\n",
        "                # Write the index to the file\n",
        "                f.write(str(i) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qhKgnhpkvSI2"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, isTest):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    all_preds1 = []\n",
        "    all_labels1 = []\n",
        "    all_preds2 = []\n",
        "    all_labels2 = []\n",
        "    all_preds3 = []\n",
        "    all_labels3 = []\n",
        "    all_preds4 = []\n",
        "    all_labels4 = []\n",
        "    all_preds5 = []\n",
        "    all_labels5 = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 in loader:\n",
        "\n",
        "            img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 = img_inps.to(device), \\\n",
        "                txt_tokens.to(device), masks.to(device), labels1.to(device), labels2.to(device), labels3.to(device), \\\n",
        "                labels4.to(device), labels5.to(device)\n",
        "\n",
        "            img_feats = clip_model.module.encode_image(img_inps)\n",
        "            txt_feats = clip_model.module.encode_text(txt_tokens)\n",
        "\n",
        "            outputs1, outputs2, outputs3, outputs4, outputs5 = model(img_feats, txt_feats, masks)\n",
        "\n",
        "            preds1 = (outputs1>0.5).int()\n",
        "            preds2 = (outputs2>0.5).int()\n",
        "            preds3 = (outputs3>0.5).int()\n",
        "            preds4 = (outputs4>0.5).int()\n",
        "            preds5 = (outputs5>0.5).int()\n",
        "            \n",
        "            test_loss += (criterion(outputs1, labels1.unsqueeze(1).float()).item() + criterion(outputs2, labels2.unsqueeze(1).float()).item() + \\\n",
        "                            criterion(outputs3, labels3.unsqueeze(1).float()).item() + criterion(outputs4, labels4.unsqueeze(1).float()).item() + \\\n",
        "                            criterion(outputs5, labels5.unsqueeze(1).float()).item())\n",
        "\n",
        "            all_preds1.extend(preds1.cpu().numpy().flatten())\n",
        "            all_labels1.extend(labels1.cpu().numpy().flatten())\n",
        "            all_preds2.extend(preds2.cpu().numpy().flatten())\n",
        "            all_labels2.extend(labels2.cpu().numpy().flatten())\n",
        "            all_preds3.extend(preds3.cpu().numpy().flatten())\n",
        "            all_labels3.extend(labels3.cpu().numpy().flatten())\n",
        "            all_preds4.extend(preds4.cpu().numpy().flatten())\n",
        "            all_labels4.extend(labels4.cpu().numpy().flatten())\n",
        "            all_preds5.extend(preds5.cpu().numpy().flatten())\n",
        "            all_labels5.extend(labels5.cpu().numpy().flatten())\n",
        "\n",
        "        acc = metrics.accuracy_score(all_labels1, all_preds1)\n",
        "        f1_1 = metrics.f1_score(all_labels1, all_preds1, average='macro')\n",
        "        f1_2 = metrics.f1_score(all_labels2, all_preds2, average='micro')\n",
        "        f1_3 = metrics.f1_score(all_labels3, all_preds3, average='micro')\n",
        "        f1_4 = metrics.f1_score(all_labels4, all_preds4, average='micro')\n",
        "        f1_5 = metrics.f1_score(all_labels5, all_preds5, average='micro')\n",
        "\n",
        "        if(isTest):\n",
        "          print(\"Performing Error Handling Next - \")\n",
        "          performErrorHandling(all_preds1, all_labels1)\n",
        "\n",
        "    return test_loss/len(loader), acc, f1_1, f1_2, f1_3, f1_4, f1_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4xL0i5Ug9n8Z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset, sampler\n",
        "train_loader = DataLoader(train_data, shuffle=True, num_workers=2, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, num_workers=2, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, num_workers=2, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "x3aYbBYbv5YT"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, lr_scheduler, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model = model\n",
        "    best_acc = 0.0\n",
        "    best_val_loss = 100\n",
        "    best_epoch = 0\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        since2 = time.time()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_samples = 0.0\n",
        "        cnt = 0\n",
        "\n",
        "        for img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 in tr_loader:\n",
        "            img_inps, txt_tokens, masks, labels1, labels2, labels3, labels4, labels5 = (\n",
        "                img_inps.to(device),\n",
        "                txt_tokens.to(device),\n",
        "                masks.to(device),\n",
        "                labels1.to(device),\n",
        "                labels2.to(device),\n",
        "                labels3.to(device),\n",
        "                labels4.to(device),\n",
        "                labels5.to(device),\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                img_feats = clip_model.module.encode_image(img_inps)\n",
        "                txt_feats = clip_model.module.encode_text(txt_tokens)\n",
        "\n",
        "            outputs1, outputs2, outputs3, outputs4, outputs5 = model(\n",
        "                img_feats, txt_feats, masks\n",
        "            )\n",
        "            preds1 = (outputs1 > 0.5).int()\n",
        "\n",
        "            loss = criterion(outputs1, labels1.unsqueeze(1).float()) + criterion(\n",
        "                outputs2, labels2.unsqueeze(1).float()\n",
        "            ) + criterion(outputs3, labels3.unsqueeze(1).float()) + criterion(\n",
        "                outputs4, labels4.unsqueeze(1).float()\n",
        "            ) + criterion(outputs5, labels5.unsqueeze(1).float())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(preds1 == labels1.data.view_as(preds1)).item()\n",
        "            total_samples += len(labels1)\n",
        "\n",
        "            if cnt % 40 == 0:\n",
        "                print(\n",
        "                    f\"[{epoch}, {cnt+1}] loss: {loss.item():.4f}, Acc: {(100.0 * running_corrects) / total_samples:.2f}\"\n",
        "                )\n",
        "\n",
        "            cnt += 1\n",
        "\n",
        "        if scheduler:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        train_loss = running_loss / len(tr_loader)\n",
        "        train_acc = running_corrects / len(tr_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Training Loss: {train_loss:.6f} Acc: {100.0 * train_acc:.2f}\"\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc, test_f1_1, test_f1_2, test_f1_3, test_f1_4, test_f1_5 = evaluate(\n",
        "            model, vl_loader, False\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, Val Loss: {test_loss:.4f}, Acc: {test_acc*100:.2f}, F1_1: {test_f1_1*100:.2f}, F1_2: {test_f1_2*100:.2f}, F1_3: {test_f1_3*100:.2f}, F1_4: {test_f1_4*100:.2f}, F1_5: {test_f1_5*100:.2f}\"\n",
        "        )\n",
        "\n",
        "        # deep copy the model\n",
        "        if test_f1_1 >= best_f1:\n",
        "            best_acc = test_acc\n",
        "            best_val_loss = test_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "            best_f1 = test_f1_1\n",
        "\n",
        "    time_elapsed2 = time.time() - since2\n",
        "    print('Epoch complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed2 // 60, time_elapsed2 % 60))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    return best_model, best_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1JXvYl9yAEYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d03d912-dadd-4eca-f6b2-4d15902c07e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMNetwork(\n",
            "  (vfc): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (tfc): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (pre_output_layer1): Linear(in_features=512, out_features=1024, bias=True)\n",
            "  (pre_output_layer2): Linear(in_features=1024, out_features=64, bias=True)\n",
            "  (cf1): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf3): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (cf5): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "2800 280\n",
            "Epoch 1/20\n",
            "----------\n",
            "[1, 1] loss: 3.4827, Acc: 54.69\n",
            "[1, 41] loss: 3.4650, Acc: 53.47\n",
            "[1, 81] loss: 3.4387, Acc: 55.77\n",
            "[1, 121] loss: 3.3400, Acc: 58.52\n",
            "Training Loss: 3.434830 Acc: 60.18\n",
            "Epoch: 1, Val Loss: 3.3473, Acc: 75.30, F1_1: 74.34, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 2/20\n",
            "----------\n",
            "[2, 1] loss: 3.3096, Acc: 71.88\n",
            "[2, 41] loss: 3.3169, Acc: 74.58\n",
            "[2, 81] loss: 3.3339, Acc: 75.35\n",
            "[2, 121] loss: 3.4164, Acc: 75.93\n",
            "Training Loss: 3.343166 Acc: 76.18\n",
            "Epoch: 2, Val Loss: 3.2888, Acc: 86.00, F1_1: 85.98, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 3/20\n",
            "----------\n",
            "[3, 1] loss: 3.3336, Acc: 75.00\n",
            "[3, 41] loss: 3.4504, Acc: 78.85\n",
            "[3, 81] loss: 3.2784, Acc: 78.80\n",
            "[3, 121] loss: 3.2966, Acc: 79.25\n",
            "Training Loss: 3.319682 Acc: 79.11\n",
            "Epoch: 3, Val Loss: 3.2769, Acc: 86.90, F1_1: 86.89, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 4/20\n",
            "----------\n",
            "[4, 1] loss: 3.3994, Acc: 73.44\n",
            "[4, 41] loss: 3.3263, Acc: 79.23\n",
            "[4, 81] loss: 3.3253, Acc: 79.55\n",
            "[4, 121] loss: 3.4066, Acc: 79.57\n",
            "Training Loss: 3.308310 Acc: 79.43\n",
            "Epoch: 4, Val Loss: 3.2685, Acc: 87.30, F1_1: 87.30, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 5/20\n",
            "----------\n",
            "[5, 1] loss: 3.2728, Acc: 81.25\n",
            "[5, 41] loss: 3.2695, Acc: 80.26\n",
            "[5, 81] loss: 3.3231, Acc: 80.48\n",
            "[5, 121] loss: 3.4141, Acc: 80.01\n",
            "Training Loss: 3.299079 Acc: 80.08\n",
            "Epoch: 5, Val Loss: 3.2667, Acc: 87.80, F1_1: 87.79, F1_2: 87.70, F1_3: 72.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 6/20\n",
            "----------\n",
            "[6, 1] loss: 3.2600, Acc: 76.56\n",
            "[6, 41] loss: 3.2997, Acc: 81.71\n",
            "[6, 81] loss: 3.2739, Acc: 80.92\n",
            "[6, 121] loss: 3.2608, Acc: 81.25\n",
            "Training Loss: 3.287133 Acc: 81.37\n",
            "Epoch: 6, Val Loss: 3.2584, Acc: 87.90, F1_1: 87.90, F1_2: 87.70, F1_3: 75.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 7/20\n",
            "----------\n",
            "[7, 1] loss: 3.2613, Acc: 79.69\n",
            "[7, 41] loss: 3.1799, Acc: 81.10\n",
            "[7, 81] loss: 3.2665, Acc: 81.33\n",
            "[7, 121] loss: 3.2716, Acc: 81.42\n",
            "Training Loss: 3.280823 Acc: 81.27\n",
            "Epoch: 7, Val Loss: 3.2467, Acc: 87.90, F1_1: 87.90, F1_2: 87.70, F1_3: 76.60, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 8/20\n",
            "----------\n",
            "[8, 1] loss: 3.2268, Acc: 84.38\n",
            "[8, 41] loss: 3.2096, Acc: 82.36\n",
            "[8, 81] loss: 3.3242, Acc: 81.46\n",
            "[8, 121] loss: 3.2458, Acc: 81.86\n",
            "Training Loss: 3.265205 Acc: 81.79\n",
            "Epoch: 8, Val Loss: 3.2433, Acc: 87.40, F1_1: 87.39, F1_2: 87.70, F1_3: 79.50, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 9/20\n",
            "----------\n",
            "[9, 1] loss: 3.2859, Acc: 76.56\n",
            "[9, 41] loss: 3.2617, Acc: 81.21\n",
            "[9, 81] loss: 3.2525, Acc: 81.98\n",
            "[9, 121] loss: 3.1900, Acc: 82.08\n",
            "Training Loss: 3.259637 Acc: 82.01\n",
            "Epoch: 9, Val Loss: 3.2370, Acc: 87.50, F1_1: 87.50, F1_2: 87.70, F1_3: 79.20, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 10/20\n",
            "----------\n",
            "[10, 1] loss: 3.2300, Acc: 89.06\n",
            "[10, 41] loss: 3.2103, Acc: 83.46\n",
            "[10, 81] loss: 3.2126, Acc: 83.06\n",
            "[10, 121] loss: 3.2535, Acc: 82.90\n",
            "Training Loss: 3.243646 Acc: 82.70\n",
            "Epoch: 10, Val Loss: 3.2351, Acc: 87.40, F1_1: 87.39, F1_2: 87.70, F1_3: 80.60, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 11/20\n",
            "----------\n",
            "[11, 1] loss: 3.2640, Acc: 84.38\n",
            "[11, 41] loss: 3.3217, Acc: 81.75\n",
            "[11, 81] loss: 3.2479, Acc: 82.29\n",
            "[11, 121] loss: 3.1858, Acc: 82.23\n",
            "Training Loss: 3.239827 Acc: 82.23\n",
            "Epoch: 11, Val Loss: 3.2327, Acc: 87.90, F1_1: 87.90, F1_2: 87.70, F1_3: 80.40, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 12/20\n",
            "----------\n",
            "[12, 1] loss: 3.2755, Acc: 82.81\n",
            "[12, 41] loss: 3.2776, Acc: 84.64\n",
            "[12, 81] loss: 3.2347, Acc: 83.72\n",
            "[12, 121] loss: 3.2034, Acc: 83.66\n",
            "Training Loss: 3.230415 Acc: 83.43\n",
            "Epoch: 12, Val Loss: 3.2328, Acc: 87.80, F1_1: 87.80, F1_2: 87.70, F1_3: 80.90, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 13/20\n",
            "----------\n",
            "[13, 1] loss: 3.2880, Acc: 78.12\n",
            "[13, 41] loss: 3.2142, Acc: 83.54\n",
            "[13, 81] loss: 3.1170, Acc: 83.68\n",
            "[13, 121] loss: 3.2001, Acc: 83.48\n",
            "Training Loss: 3.227278 Acc: 83.40\n",
            "Epoch: 13, Val Loss: 3.2311, Acc: 88.00, F1_1: 88.00, F1_2: 87.70, F1_3: 81.00, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 14/20\n",
            "----------\n",
            "[14, 1] loss: 3.1705, Acc: 87.50\n",
            "[14, 41] loss: 3.2572, Acc: 82.09\n",
            "[14, 81] loss: 3.2471, Acc: 82.39\n",
            "[14, 121] loss: 3.2703, Acc: 82.99\n",
            "Training Loss: 3.227648 Acc: 83.01\n",
            "Epoch: 14, Val Loss: 3.2322, Acc: 88.00, F1_1: 88.00, F1_2: 87.70, F1_3: 80.60, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 15/20\n",
            "----------\n",
            "[15, 1] loss: 3.2342, Acc: 75.00\n",
            "[15, 41] loss: 3.0889, Acc: 83.42\n",
            "[15, 81] loss: 3.2884, Acc: 83.85\n",
            "[15, 121] loss: 3.2371, Acc: 83.81\n",
            "Training Loss: 3.220201 Acc: 83.69\n",
            "Epoch: 15, Val Loss: 3.2309, Acc: 88.10, F1_1: 88.10, F1_2: 87.70, F1_3: 80.70, F1_4: 77.20, F1_5: 90.00\n",
            "Epoch 16/20\n",
            "----------\n",
            "[16, 1] loss: 3.2006, Acc: 81.25\n",
            "[16, 41] loss: 3.1601, Acc: 84.22\n",
            "[16, 81] loss: 3.2470, Acc: 83.64\n",
            "[16, 121] loss: 3.2527, Acc: 83.70\n",
            "Training Loss: 3.215946 Acc: 83.72\n",
            "Epoch: 16, Val Loss: 3.2313, Acc: 87.60, F1_1: 87.60, F1_2: 87.70, F1_3: 80.70, F1_4: 77.30, F1_5: 90.00\n",
            "Epoch 17/20\n",
            "----------\n",
            "[17, 1] loss: 3.2094, Acc: 82.81\n",
            "[17, 41] loss: 3.2545, Acc: 83.23\n",
            "[17, 81] loss: 3.1101, Acc: 83.68\n",
            "[17, 121] loss: 3.2254, Acc: 83.91\n",
            "Training Loss: 3.212577 Acc: 83.89\n",
            "Epoch: 17, Val Loss: 3.2312, Acc: 87.70, F1_1: 87.70, F1_2: 87.70, F1_3: 80.60, F1_4: 78.20, F1_5: 90.00\n",
            "Epoch 18/20\n",
            "----------\n",
            "[18, 1] loss: 3.1807, Acc: 84.38\n",
            "[18, 41] loss: 3.2018, Acc: 83.38\n",
            "[18, 81] loss: 3.1467, Acc: 83.47\n",
            "[18, 121] loss: 3.2021, Acc: 83.64\n",
            "Training Loss: 3.215178 Acc: 83.57\n",
            "Epoch: 18, Val Loss: 3.2322, Acc: 87.60, F1_1: 87.60, F1_2: 87.70, F1_3: 80.90, F1_4: 78.40, F1_5: 90.00\n",
            "Epoch 19/20\n",
            "----------\n",
            "[19, 1] loss: 3.1859, Acc: 85.94\n",
            "[19, 41] loss: 3.1962, Acc: 83.27\n",
            "[19, 81] loss: 3.2473, Acc: 83.91\n",
            "[19, 121] loss: 3.3087, Acc: 84.04\n",
            "Training Loss: 3.208476 Acc: 84.02\n",
            "Epoch: 19, Val Loss: 3.2310, Acc: 87.50, F1_1: 87.50, F1_2: 87.70, F1_3: 80.90, F1_4: 80.60, F1_5: 90.00\n",
            "Epoch 20/20\n",
            "----------\n",
            "[20, 1] loss: 3.2487, Acc: 84.38\n",
            "[20, 41] loss: 3.2179, Acc: 85.48\n",
            "[20, 81] loss: 3.2593, Acc: 84.65\n",
            "[20, 121] loss: 3.2128, Acc: 84.16\n",
            "Training Loss: 3.207120 Acc: 84.09\n",
            "Epoch: 20, Val Loss: 3.2263, Acc: 87.70, F1_1: 87.70, F1_2: 87.70, F1_3: 81.20, F1_4: 82.00, F1_5: 90.00\n",
            "Epoch complete in 8m 19s\n",
            "Training complete in 179m 34s\n",
            "--- 10774.010909557343 seconds ---\n",
            "Validation best epoch: 15, Val Loss: 3.2309, ACC: 88.10, F1: 88.10\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "dim = clip_dim[vmodel]\n",
        "# Data Loaders\n",
        "tr_loader = DataLoader(train_data, shuffle=True, num_workers=4, batch_size=batch_size)\n",
        "vl_loader = DataLoader(valid_data, num_workers=2, batch_size=batch_size)\n",
        "ts_loader = DataLoader(test_data, num_workers=2, batch_size=batch_size)\n",
        "\n",
        "# Model\n",
        "model = MMNetwork(dim, dim, 1).to(device)\n",
        "print(model)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), init_lr, betas=(0.99, 0.98), weight_decay=1e-4)\n",
        "criterion = nn.BCELoss()\n",
        "num_train_steps = len(train_data) // batch_size * epochs\n",
        "num_warmup_steps = int(0.1 * num_train_steps)\n",
        "print(num_train_steps, num_warmup_steps)  # Print Number of total and warmup steps\n",
        "\n",
        "scheduler = MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.5)\n",
        "import time\n",
        "start_time = time.time()\n",
        "model_ft, best_epoch = train(model, optimizer, scheduler, num_epochs=epochs)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# torch.save(model_ft.state_dict(), 'saved_models/trained_model_%s.pt'%(args.net, vmodel))\n",
        "\n",
        "vl_loss, vl_acc, vl_f1, _, _, _, _ = evaluate(model_ft, vl_loader, False)\n",
        "print('Validation best epoch: %d, Val Loss: %.4f, ACC: %.2f, F1: %.2f' % (best_epoch, np.round(vl_loss, 4), vl_acc*100, vl_f1*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PKViEIrXwUYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0e20fe-4c5d-46ec-b6ca-225c9ed8f5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Error Handling Next - \n",
            "Test results:, Test Loss: 3.5288, ACC: 75.20, F1- Task A: 74.23, F1 - Task B: 76.38\n"
          ]
        }
      ],
      "source": [
        "ts_loss, ts_acc, ts_f1, ts_f2, ts_f3, ts_f4, ts_f5 = evaluate(model_ft, test_loader, True)\n",
        "taskB_F1 = (ts_f2 + ts_f3 + ts_f4 + ts_f5)/4\n",
        "print('Test results:, Test Loss: %.4f, ACC: %.2f, F1- Task A: %.2f, F1 - Task B: %.2f'%(np.round(ts_loss,4), ts_acc*100, ts_f1*100, taskB_F1*100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}